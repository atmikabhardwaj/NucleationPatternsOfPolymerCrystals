'''This file is a part of the work 'Nucleation patterns of polymer crystals analyzed by machine learning models' and is written by Atmika Bhardwaj (bhardwaj@ipfdd.de) and Marco Werner (werner-marco@ipfdd.de). 

Here, we define functions such as reading dump files generated by LAMMPS and training an AutoEncoder. We start by reading a coordinate file (dump file from Lammps) and analyze it to study the local environmental information of every coarse-grained bead. Then, we train an AE to compress that information and train a GMM on the compressed data to classify each coarse-grained into amorphous or crytalline depending on its enviromental fingerprint.'''

import numpy as np
import random
import time
import os
import tensorflow as tf
import keras
from keras.layers       import Input, Dense
from keras.models       import Model
from keras.regularizers import l2
from keras.callbacks    import EarlyStopping
from keras.callbacks    import ReduceLROnPlateau
from keras              import backend as K
import collections
import numpy as np

class ReadCoordinates:
    
    def __init__(self, fileName="", divideChains = 0):
        
        self.readTimeStep = 'ITEM: TIMESTEP'
        self.readN        = 'ITEM: NUMBER OF ATOMS'
        self.readBox      = 'ITEM: BOX BOUNDS'
        self.readAtoms    = 'ITEM: ATOMS'
        self.divideChains = divideChains
        
        if ( fileName != "" ):
            self.readFile(fileName)
        
    def readTotalAtoms(self):
        self.fileReader.seek(0)
        for i in range(10):
            if self.readN in self.fileReader.readline():
                self.totalAtoms = int(self.fileReader.readline())
            
    def readBoxSize(self):

        Dx=[]
        Dy=[]
        Dz=[]

        self.fileReader.seek(0)
        for i in range(10):
            if self.readBox in self.fileReader.readline():
                Dx = [float(bx) for bx in self.fileReader.readline().split()]
                Dy = [float(by) for by in self.fileReader.readline().split()]
                Dz = [float(bz) for bz in self.fileReader.readline().split()]
                
        self.boxMin = np.array((Dx[0], Dy[0], Dz[0]))
        self.boxMax = np.array((Dx[1], Dy[1], Dz[1]))
        
        self.boxDimension = self.boxMax - self.boxMin
        
    def readCoordinates(self, numCoordinates):

        X = []
        self.fileReader.seek(0)

        while self.readAtoms not in self.fileReader.readline():
            print()

        for i in range(numCoordinates):
            X.append([float(string) for string in self.fileReader.readline().split()])
               
        X = np.array(X)
        X = X[X[:, 0].argsort()]
        
        self.atomID   = np.int64(X[:, 0])
        self.molID    = np.int64(X[:, 1])
        
        containsJumps = X.shape[1]>5
        
        if ( containsJumps ):
            self.folded   = X[:, 2:5]                                    #projected to the box, broken
            self.jumps    = X[:, 5:]                                     #with broke chains
            self.absolute = self.folded + self.boxDimension*self.jumps   #not broken and not projected
        else :
            print ("alert no jumps")
            exit()
        
        if ( self.divideChains ):
            self.divideintochains()
        
    def readFile(self,fileName):
        
        self.fileName   = fileName
        self.fileReader = open(fileName, 'r')       
        self.readTotalAtoms()
        self.readBoxSize()
        self.readCoordinates(self.totalAtoms)
        self.calculateBondVectors()
    
    def divideintochains(self):
        
        chainLength =  self.divideChains
        for i in np.arange(int(self.totalAtoms/chainLength)):
            temp = i*chainLength
            self.molID[temp:temp+chainLength] = i+1

    def calculateBondVectors(self):

        allbonds = []
        self.bonds = []

        for i in set(self.molID):
            temp      = self.absolute[self.molID == i]
            temp1     = calcPeriodicDistance_bonds(temp, self.boxDimension )
            temp2     = temp1[-1]
            allbonds  = np.vstack([temp1, temp2])
            normalise = np.linalg.norm(allbonds, axis = 1, keepdims = True)
            allbonds *= 1./normalise
            self.bonds.append(allbonds)
        self.bonds    = np.concatenate(self.bonds)
        
def projectToBox(xyz, box):

    L            = box[:3]
    offset       = box[3:]
    xyzProjected = xyz.copy()
    
    for i in np.arange(3):
        
        c        = xyzProjected[:, i]
        L_       = L[i]
        o_       = offset[i]
        bits     = c < o_
        while (np.sum(bits) > 0):
            c   += bits*L_
            bits = c < o_
            
        bits     = c > o_ + L_
        while (np.sum(bits) > 0):
            c   -= bits*L_
            bits = c > o_ + L_
            
        xyzProjected[:, i] = c
        
    return xyzProjected

def calcPeriodicDistance_bonds(r1, L):
    
    temp          = r1[:-1]
    for i in range(len(r1[0])):
        
        d1        = np.diff(r1[:, i], axis=0)
        d2        = d1 + L[i]
        d3        = d1 - L[i]
        darray    = np.vstack((d1, d2, d3))
        dmin      = np.argmin(np.abs(darray), axis = 0)
        newlist   = [darray[ele][ind]for ind, ele in enumerate(dmin)]
        temp[:,i] = newlist

    return temp

def calcSqrPeriodicDistance(r1, r2, L):

    d1 = r2 - r1
    d2 = d1 + L
    d3 = d1 - L
    darray = np.vstack((d1*d1, d2*d2, d3*d3))
    dmin   = np.min(darray, axis=0)
    return dmin
    
def calcDistances(Rall, r, box):

    x = Rall[:, 0]
    y = Rall[:, 1]
    z = Rall[:, 2]
    distances = np.sqrt(calcSqrPeriodicDistance(x, r[0], box[0])
                    +   calcSqrPeriodicDistance(y, r[1], box[1])
                    +   calcSqrPeriodicDistance(z, r[2], box[2]))
    return distances

class GridSearch:
    
    def __init__(self, obj, averageNumPerCell=4., numSigma = 3.0):
        
        xyz = obj.folded
        box = np.array(list(obj.boxDimension) + list(obj.boxMin))
        maxNumPerCell = int(np.rint(averageNumPerCell + numSigma*np.sqrt(averageNumPerCell)) + 0.1) #based on average and standard deviation

        print ("defined max num. per cell = ",maxNumPerCell)    

        volume       = box[0]*box[1]*box[2]
        N            = xyz.shape[0]                   # # of particles
        dL           = ( volume / (N/averageNumPerCell) )**(1./3.)   # cellsize

        print ("dL (estimate) = ", dL)    
        gridDim      = (box[:3] / dL ).astype(np.intc) # number of grid cells [in_x, in_y, in_z]
        dL           =  box[:3] / gridDim              # final cell size in x, y, z
        print ("dL (final) = ", dL)

        print ("num grid cells = ", gridDim )

        # listGrid means a grid of lists of particles found in each cell
        # its dimension is the grid dimension + the estimated max. number of entries. 
        listGridDim  = np.concatenate ((gridDim, np.array((maxNumPerCell,)) )) # z.B. (2, 2, 2, 5); 5 = no. of particles in one grid-cell
        listGrid     = np.ones(listGridDim).astype(np.intc)*-1                 # empty cells have a "-1" as default value

        # this helper array stores the number of entries is each cell
        occupancies  = np.zeros(gridDim).astype(np.intc)              # z.B. (2, 2, 2)

        print ("list grid shape = ", listGrid.shape) 
        print ("list grid size in MB:", listGrid.nbytes/(1024*1024))

        print ("occupancy shape = ", occupancies.shape)
        print ("occupancy grid size in MB:", occupancies.nbytes/(1024*1024))

        epsilon      = 1e-6
        xyzProjected = projectToBox(xyz, box)                        # returns obj.folded as is (extra periodicity check)
        gridIdx      = ((xyzProjected - box[3:])/dL).astype(np.intc) # 3d grid indices for each particle

        print (gridIdx[:10])
        # filling the grid by each particle:
        print ("filling the grid by each particle:")
        
        for idx in np.arange(gridIdx.shape[0]):
            
            if ( idx % ( gridIdx.shape[0] // 5 ) == 0):
                print ("progress / % : ",100*idx/gridIdx.shape[0])
            cell     = gridIdx[idx]
            
            if ( np.sum (cell == gridDim) > 0 ):                    # array([False, False, False])
                print ("pos = ", (xyz[idx] - box[3:]), " cell = ", idx)
                print ("xyz[idx] = ", xyz[idx])
                print ("box[3:]  = ", box[3:])

            occ      = occupancies[cell[0], cell[1], cell[2]]       # no. of particles in that grid-cell
            
            if (occ >= maxNumPerCell):
                print ("cannot append element ", idx, ", xyz = ", xyz[idx], "already found elements = ", listGrid[cell[0], cell[1], cell[2]])
                print ("Please increase the numSigma value.")
            else:
                listGrid   [cell[0], cell[1], cell[2], occ] = idx   # idx goes to the cell
                occupancies[cell[0], cell[1], cell[2]]      = occ+1 # +1 particles in that grid-cell

        '''s = listGrid.shape                   # z.B. (2, 2, 2, 5)

        # Creating a supergrid holding forward and backward periodic copies of the grid. 
        # At expense of memory, this simplifies managing periodic boundaries and 
        # shall speed up slicing for the search. Size of the supergrid is 3x3x3 of the grid size.
        supershape      = np.array(s)
        supershape[:3] *= 3                  # z.B. (6, 6, 6, 5)
        print ("supershape = ",supershape)
        supergrid       = np.ones(supershape).astype(np.intc)*-1    # z.B. (6, 6, 6, 5)

        # Creating the periodic copies of original grid
        for i in np.arange(3):
            for j in np.arange(3):
                for k in np.arange(3):
                    supergrid[i*s[0]:(i+1)*s[0],j*s[1]:(j+1)*s[1],k*s[2]:(k+1)*s[2],:] = listGrid'''
                    
        supergrid = np.tile(listGrid, (3, 3, 3, 1))
        # repeat listgrid 3 times along first three dim. keeping fourth dim unchanged
        
        print ("filling supergrid")
        print ("supergrid size in MB:", supergrid.nbytes/(1024*1024))
        
        self.box            = box
        self.gridDim        = gridDim
        self.gridIdx        = gridIdx
        self.supergrid      = supergrid
        self.dL             = dL
        self.numPerCell     = maxNumPerCell
        self.totalOccupancy = np.sum(occupancies)
        self.maxOccupancy   = np.max(occupancies)
        
        self.indices        = []
        self.distances      = []
        self.queryList      = []
        self.Rmax           = 0.
        
        print ("max occupancy = ",   self.maxOccupancy)
        print ("total occupancy = ", np.sum( supergrid>-0.5 ), N*27 )
        

    def findNeighbors(self, xyz, queryPos, Rmax, verb=False):
        
        L         = self.box[:3]
        boxOffset = self.box[3:]
        
        if ( verb ) :
            print ("dL, L = ", L, self.dL, queryPos, boxOffset )
            
        # max and min grid cell indices that radius Rmax will cover
        idxMax    = (( ( (queryPos-boxOffset) + Rmax + L ) / self.dL).astype(np.intc))
        idxMin    = (( ( (queryPos-boxOffset) - Rmax + L ) / self.dL).astype(np.intc))
        
        if ( verb ) :
            print ( "idx min and max =  ", idxMax, idxMin )

        # subgrid extracted containing all the cells which might contain neighbors within Rmax of queryPos
        subgrid   = self.supergrid [idxMin[0]:idxMax[0]+1, idxMin[1]:idxMax[1]+1, idxMin[2]:idxMax[2]+1, :]

        if ( verb ) :
            print (" subgrid size ", subgrid.size)

        selectsub = subgrid.reshape(subgrid.size)  # filtered indices flattened into a 1D array
        selectsub = selectsub[selectsub >= 0]      # negative indices removed
        
        matchingPositions = xyz[selectsub]
        
        if ( verb ) :
            print ( "matchingPositions ", matchingPositions.shape )
        dist      = calcDistances( matchingPositions, queryPos, L )
        select    = dist < Rmax
        
        return selectsub[select], dist[select]

    def updateNeighborLists(self, obj, queryList, Rmax):
        
        xyz = obj.folded
        box = obj.boxDimension
        
        print ( "Old and new Rmax are ", self.Rmax, Rmax)
        
        self.queryList     = queryList
        
        if ( (( Rmax - self.Rmax )**2)**0.5 > 1e-6 ):
            
            print ( "... updating neighbor lists!")
        
            queryPositions = xyz[queryList].copy()
            
            queryPositions = projectToBox(queryPositions, self.box)
            
            # Estimate average and maximum number of neighbors
            avgNumber   = 4.*np.pi/3.*Rmax**3*(self.totalOccupancy/(box[0]*box[1]*box[2]))
            print ("avgNumber = ",avgNumber)
            maxNum      = avgNumber
            estimateMax = int (avgNumber + 3*np.sqrt(avgNumber))
                    
            idx            = 0
            self.indices   = []
            self.distances = []
            verbose        = False
            
            for queryPos in queryPositions:
            
                nlist, dlist = self.findNeighbors( xyz, queryPos, Rmax, verbose )
                verbose      = False
                
                self.indices.append(nlist)
                self.distances.append(dlist)
                                
                if ( idx % ( queryPositions.shape[0] // 20 ) == 0):
                    print ("progress / % : ", 100*idx/queryPositions.shape[0])
                    verbose = True
                idx  += 1
        
            self.Rmax = Rmax
            
        else:
            print ( "... NOT updating neighbor lists!")

def normg(av):

    bins    = len(av)
    j       = np.arange(bins+1)
    v_shell = np.pi*(4./3.)*(j[1:bins+1]**3-j[:bins]**3)
    rho     = av/v_shell
    rho    /= np.mean(rho[len(rho)-10:])
    return rho[1:]            

def analyzePCV2(grid, dr):

    counts = []
    idxMin = int(0.8/dr)
    
    for d in grid.distances:
        c  = np.histogram (d, bins = int(grid.Rmax/dr), range=(0, grid.Rmax))
        counts.append(c[0])

    counts = np.array(counts)
    
    g_vol  = list()
    for i in np.arange(len(counts)):
        temp = np.array(normg(counts[i]))/ dr**3
        g_vol.append(temp[idxMin:])
        
    g_vol  = np.array(g_vol)
    g_vol /= np.mean(g_vol[:, g_vol.shape[1]-2:])
    
    return g_vol

def avgor(gridsearch, obj, dr=0):
    
    dists   = gridsearch.distances
    indices = gridsearch.indices
    Rmax    = gridsearch.Rmax
    rnd     = gridsearch.queryList
    bonds   = obj.bonds   
    fb      = []
    
    for i in np.arange(len(rnd)):
        fb.append(bonds[indices[i]])

    b     = bonds[rnd].copy()
    dprod = []
    for i in np.arange(len(rnd)):
        dprod.append( 1.5 * (np.sum(b[i] * fb[i], axis=1) **2) - 0.5)

    if dr:
        dp       = np.zeros((len(rnd), int(Rmax/dr)))
        counts_s = np.zeros((len(rnd), int(Rmax/dr)))
        for i in np.arange(len(rnd)):
            if ( i % 10000 == 0 ): print ( "rdf for particle ", i) 
            dists_s   = np.int64(np.array(dists[i])/dr)
            for j in np.arange(int(Rmax/dr)):
                dp_bb = np.array(dprod[i])[dists_s==j]
                if len(dp_bb):                          # sometimes there is no particle in the shell
                    dp[i, j]   = np.mean(dp_bb)
                counts_s[i, j] = len(dp_bb)
        return dp, counts_s
    
    else:
        dp = []
        for i in np.arange(len(rnd)):
            dp.append(np.mean(dprod[i], axis = 0))            
            if ( i % 10000 == 0 ): print ( "rdf for particle ", i)
        dp = np.array(dp)
        return dp
    
def avgor2(gridsearch, obj, dr=0):
    
    dists   = gridsearch.distances
    indices = gridsearch.indices
    Rmax    = gridsearch.Rmax
    rnd     = gridsearch.queryList
    bonds   = obj.bonds
    
    #fb      = []
    
    #for i in np.arange(len(rnd)):
        #fb.append(bonds[indices[i]])
    b     = bonds[rnd].copy()
    
    if ( dr ) :
    
        num_r = int(Rmax/dr)
        
        dp       = np.zeros((len(rnd), num_r ) )
        counts_s = np.zeros((len(rnd), num_r ) )
        
        #dprod = []
        for i in np.arange(len(rnd)):
            if ( i % 10000 == 0 ): print ( "rdf for particle ", i) 
            
            dprod =  1.5 * (np.sum(b[i] * bonds[indices[i]], axis=1) **2) - 0.5
            
            r_idx = np.int64(np.array(dists[i])/dr)
            
            inflated_sum=np.zeros((len(dprod),num_r+1))
            inflated_count=np.zeros((len(dprod),num_r+1)).astype(int)
            
            inflated_sum[np.arange(len(dprod)),r_idx]=dprod
            inflated_count[np.arange(len(dprod)),r_idx]=1
            
            cnt_r=np.sum(inflated_count,axis=0)
            avg_r=np.sum(inflated_sum,axis=0)
            
            where_gtr0=cnt_r>0.5
            avg_r[where_gtr0]/=cnt_r[where_gtr0]
            
            dp[i,:]=avg_r[:num_r]
            counts_s[i,:]=cnt_r[:num_r]
            
#            for i in np.arange(len(rnd)):
#                
#                dists_s   = np.int64(np.array(dists[i])/dr)
#                for j in np.arange(int(Rmax/dr)):
#                    dp_bb = np.array(dprod[i])[dists_s==j]
#                    if len(dp_bb):                          # sometimes there is no particle in the shell
#                        dp[i, j]   = np.mean(dp_bb)
#                    counts_s[i, j] = len(dp_bb)
        return dp, counts_s
    
    else:
        dp = []
        dprod =  1.5 * (np.sum(b[i] * bonds[indices[i]], axis=1) **2) - 0.5
        for i in np.arange(len(rnd)):
            dp.append(np.mean(dprod, axis = 0))            
            if ( i % 10000 == 0 ): print ( "rdf for particle ", i)
        dp = np.array(dp)
        return dp
    

def calculatebondsarray(obj, limit):

    mol              = obj.molID
    combinations     = 2*limit
    all_combinations = []

    for j in range(1, combinations+1):
        temp4     = []
        for i in set(mol):
            temp  = obj.bonds[mol == i]
            a     = temp[:-j]
            b     = temp[j:]
            temp1 = 1.5 * ( np.sum(a * b, axis=1) **2) - 0.5
            temp2 = temp1[-j:]
            temp3 = np.concatenate([temp1,temp2])
            temp4.append(temp3)                    # 1000*1000
            
        all_combinations.append(temp4)             # 4*1000*1000
    selectedbonds = []
    temp11        = []

    for k in set(mol):
        temp5 = all_combinations[0][k-1]           # 1000
        temp7 = []
        for l in np.arange(len(temp5)- combinations+1):
            temp6  = []
            m      = 0
            for j in np.arange(combinations):
                for i in range(0, limit*2-j):
                    temp6.append(all_combinations[m][k-1][i+l])
                m += 1
            temp7.append(temp6)

        temp8  = temp7[:limit]
        temp9  = temp7[-(limit-1):]
        temp10 = np.concatenate([temp8, temp7, temp9])
        temp11.append(temp10)

    temp12 = list(np.concatenate(temp11))
    selectedbonds.append(temp12)
    return selectedbonds[0]

def scalingfunct(array):

    maxval = np.max(array, axis = 0)
    for i in np.arange(len(array[0])):
        array[:,i] *= 1/maxval[i]
    return array

def flatten(x):

    if isinstance(x, collections.abc.Iterable):
        return [a for i in x for a in flatten(i)]
    else:
        return [x]

def get_slice_indices(obj, sections, slicenum, axis):     # along the axis-coordinate
    
    arr   = obj.folded
    zmin  = min(arr[:, axis])
    zmax  = max(arr[:, axis])
    width = (zmax - zmin)/sections
    zi    = zmin + (width * slicenum)
    zj    = zi + width
    temp  = np.logical_and(zi <= arr[:, axis], arr[:, axis] < zj)
    return np.arange(len(arr))[temp]

def get_slice_indx(obj, number_cuts, slicenum, cut_axis):       # another function to cut along the cut_axis-coordinate
    
    arr         = obj.folded
    cx          = arr[:, cut_axis]
    cuts        = np.linspace (min(cx), max(cx), number_cuts)
    indices     = np.nonzero (np.logical_and( cuts[slicenum] <= cx, cx < cuts[slicenum + 1]))    
    return np.arange(len(arr))[indices]

def fixedSeeds(bneurons, width, x, depth, bulkAct, L2, VL, nEpochs, LR, validationSplit, batchSize, savingfile):
    
    seed_value     = 0
    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value
    os.environ['PYTHONHASHSEED']=str(seed_value)
    # 2. Set the `python` built-in pseudo-random generator at a fixed value
    random.seed(seed_value)
    # 3. Set the `numpy` pseudo-random generator at a fixed value
    np.random.seed(seed_value)
    # 4. Set the `tensorflow` pseudo-random generator at a fixed value
    tf.random.set_seed(seed_value)
    h              = []
    reg            = tf.keras.regularizers.l2(L2)
    input_dim      = Input(shape = (x.shape[1],))
    rlrop          = ReduceLROnPlateau(monitor = 'val_loss', patience = 25, mode    = 'min', factor  = 0.5)
    
    layer          = Dense(width,      activation = bulkAct, kernel_regularizer = reg, use_bias = False)(input_dim)
    for lnum in np.arange(1,depth):
        layer      = Dense(width,      activation = bulkAct, kernel_regularizer = reg, use_bias = False)(layer)
        
    bottleneck     = Dense(bneurons,   activation = bulkAct, kernel_regularizer = reg, use_bias = False)(layer)
    layer          = Dense(width,      activation = bulkAct, kernel_regularizer = reg, use_bias = False)(bottleneck)
    
    for lnum in np.arange(1,depth):
        layer      = Dense(width,      activation = bulkAct, kernel_regularizer = reg, use_bias = False)(layer)
        
    decoded        = Dense(x.shape[1], activation = bulkAct, kernel_regularizer = reg, use_bias = True)(layer)
    
    autoencoder    = Model(inputs = [input_dim,], outputs = decoded)

    Opt            = tf.keras.optimizers.Adam(LR)
    autoencoder.compile(optimizer = Opt, loss = "mse")

    h              = autoencoder.fit(x = x,    y = x, verbose = VL, epochs = nEpochs, shuffle = True, batch_size = batchSize, 
                                    validation_split = validationSplit, callbacks=[rlrop])
    
    encoder        = Model(input_dim, bottleneck)
    encoded_output = encoder.predict(x)
    encoder.save(savingfile)
    print(encoded_output[:5])
    return autoencoder, encoder, h, encoded_output

    

